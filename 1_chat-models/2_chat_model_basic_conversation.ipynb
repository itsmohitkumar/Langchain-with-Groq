{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#langchain Documentation:-\n",
    "#[ChatGroq]: https://python.langchain.com/v0.2/docs/integrations/chat/groq/\n",
    "\n",
    "# LangChain Expression Language (LCEL): https://python.langchain.com/v0.1/docs/modules/model_io/chat/quick_start/\n",
    "\n",
    "#LLM Api Documentation:-\n",
    "#[Groq]: https://console.groq.com/docs/quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU langchain-groq python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library's\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things Used:\n",
    "\n",
    "# AIMessage: Used to handle messages from the AI. \n",
    "# HumanMessage: Used to handle messages from the human user.\n",
    "# SystemMessage: Used to handle system-level instructions or context that guide the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Groq model\n",
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\" #choose model from the list:https://console.groq.com/docs/models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SystemMessage: Message for priming AI behavior, usually passed in as the first of a sequenc of input messages.\n",
    "# HumanMessagse: Message from a human to the AI model.\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is root square of pi?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from AI: The square root of a number is a value that, when multiplied by itself, gives the original number. The square root of pi (Ï€) is an irrational number, which means it cannot be expressed as a simple fraction, and it has an infinite number of decimal places.\n",
      "\n",
      "The approximate value of the square root of pi, rounded to two decimal places, is 1.77.\n",
      "\n",
      "It's important to note that the exact value of the square root of pi cannot be precisely calculated, as pi is an irrational number itself. The value of 1.77 is just an approximation.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the llm with messages\n",
    "result = llm.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIMessage: Message from an AI.\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is root square of pi?\"),\n",
    "    AIMessage(content=\"The square root of pi is rounded to two decimal places is 1.77..\"),\n",
    "    HumanMessage(content=\"What is 10 times 5?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from AI: 10 times 5 is equal to 50.\n"
     ]
    }
   ],
   "source": [
    "# Invoke the llm with messages\n",
    "result = llm.invoke(messages)\n",
    "print(f\"Answer from AI: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
